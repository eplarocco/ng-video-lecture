{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4e6b0ae-c968-44f7-ad4b-78c478cac695",
   "metadata": {},
   "source": [
    "# Task\n",
    "#### Trian a nanoGPT model on trump rally transcriptions to create likely ridiculous text in Trump's speech style\n",
    "\n",
    "# Challenges Faced\n",
    "- I first tried to train it on the onion article titles but the dataset was only ~30,000 titles and titles are very short so the limited amount of training data made for pretty terrible output especially since the whole point of the article titles are to be unpredictable and out-there. So I figured since Trump's vocabulary is pretty limited and predictable, it might make for an easier data set for the model to train on. Addiitonally there were many transcribed speaches that were very long so much more data to use.\n",
    "- Not necessarily a challenge but a limitation of the task - tokenizing by character instead of by word provides many opportunities to generate words that don't exist in the text the model was trained on or even in the English language\n",
    "- There's no clear stopping point for the generated text except when it runs to the end of the max character limit that was set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1d5c01c-a0f3-49d0-8df0-232b2f5bae5a",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Dataset Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "47c2c86a-f08e-43ef-8990-2d5c03b77040",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please provide your Kaggle credentials to download this dataset. Learn more: http://bit.ly/kaggle-creds\n",
      "Your Kaggle username:"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "  eileanorplarocco\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your Kaggle Key:"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "  ········\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset URL: https://www.kaggle.com/datasets/christianlillelund/donald-trumps-rallies\n",
      "Downloading donald-trumps-rallies.zip to ./donald-trumps-rallies\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 720k/720k [00:00<00:00, 38.1MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#!pip3 install opendatasets\n",
    "import opendatasets as od\n",
    "od.download('https://www.kaggle.com/datasets/christianlillelund/donald-trumps-rallies')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "702cdb39-ad4d-48e9-8eb7-ad52aafcc074",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def concatenate_text_files(folder_path):\n",
    "    \"\"\"Concatenates all text files in the given folder into a single string.\"\"\"\n",
    "\n",
    "    result = \"\"\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith(\".txt\"):\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "            with open(file_path, \"r\") as f:\n",
    "                result += f.read() + \"\\n\"  # Add a newline between files\n",
    "    return result\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    folder_path = \"./donald-trumps-rallies\"  # Replace with the actual path\n",
    "    concatenated_text = concatenate_text_files(folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0f25a365-42da-4a3b-9a61-c9a585ff32f7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture output\n",
    "print(concatenated_text)\n",
    "with open('trump.txt', 'w') as f:\n",
    "    f.write(concatenated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b8c8f65-e2ac-4f44-8b0e-0db45038da2d",
   "metadata": {},
   "source": [
    "# Train GPT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4fbdd7c3-8d55-4e9e-8f93-215009593ec4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.804309 M parameters\n",
      "step 0: train loss 4.5263, val loss 4.5273\n",
      "step 500: train loss 1.5882, val loss 1.5997\n",
      "step 1000: train loss 1.1701, val loss 1.1889\n",
      "step 1500: train loss 1.0394, val loss 1.0713\n",
      "step 2000: train loss 0.9514, val loss 1.0060\n",
      "step 2500: train loss 0.8931, val loss 0.9648\n",
      "step 3000: train loss 0.8433, val loss 0.9339\n",
      "step 3500: train loss 0.8057, val loss 0.9160\n",
      "step 4000: train loss 0.7721, val loss 0.9022\n",
      "step 4500: train loss 0.7465, val loss 0.8944\n",
      "step 4999: train loss 0.7162, val loss 0.8848\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# hyperparameters\n",
    "batch_size = 64 # how many independent sequences will we process in parallel?\n",
    "block_size = 256 # what is the maximum context length for predictions?\n",
    "max_iters = 5000\n",
    "eval_interval = 500\n",
    "learning_rate = 3e-4\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "eval_iters = 200\n",
    "n_embd = 384\n",
    "n_head = 6\n",
    "n_layer = 6\n",
    "dropout = 0.2\n",
    "# ------------\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "# data\n",
    "with open('trump.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "# here are all the unique characters that occur in this text\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "# create a mapping from characters to integers\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
    "\n",
    "# Train and test splits\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "# data loading\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "class Head(nn.Module):\n",
    "    \"\"\" one head of self-attention \"\"\"\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # input of size (batch, time-step, channels)\n",
    "        # output of size (batch, time-step, head size)\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x)   # (B,T,hs)\n",
    "        q = self.query(x) # (B,T,hs)\n",
    "        # compute attention scores (\"affinities\")\n",
    "        wei = q @ k.transpose(-2,-1) * k.shape[-1]**-0.5 # (B, T, hs) @ (B, hs, T) -> (B, T, T)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
    "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
    "        wei = self.dropout(wei)\n",
    "        # perform the weighted aggregation of the values\n",
    "        v = self.value(x) # (B,T,hs)\n",
    "        out = wei @ v # (B, T, T) @ (B, T, hs) -> (B, T, hs)\n",
    "        return out\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
    "\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(head_size * num_heads, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out\n",
    "\n",
    "class FeedFoward(nn.Module):\n",
    "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedFoward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "class GPTLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "        # better init, not covered in the original GPT video, but important, will cover in followup video\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
    "        x = tok_emb + pos_emb # (B,T,C)\n",
    "        x = self.blocks(x) # (B,T,C)\n",
    "        x = self.ln_f(x) # (B,T,C)\n",
    "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop idx to the last block_size tokens\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx_cond)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx\n",
    "\n",
    "model = GPTLanguageModel()\n",
    "m = model.to(device)\n",
    "# print the number of parameters in the model\n",
    "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
    "\n",
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for iter in range(max_iters):\n",
    "\n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "#Save model\n",
    "torch.save(model, 'model.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35194c14-7e52-4ec1-bef7-47d74f46968a",
   "metadata": {},
   "source": [
    "# Generate Trump Speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "743cf905-2ac5-440f-bb60-9c804ba6b22a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Saved Model\n",
    "#model = torch.load('model.pth', weights_only=False)\n",
    "#m = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0c7cb8e0-3cdb-4354-9287-fbdc90121234",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Republicans that fight on hear the 401ks that don't treat that? Go out a little bit out. One man. I have listening a friend of mine he'll start very. Then she looks I've gotten through ICE and she looks Elizabeth Laxa. When she gave crooked. Your business are paying Glg Michigan, y Glen, Mike Greenn Daylhaney. I gave you a good relaxt. I had to go. Great senators, Gretta, great job, Romcrant. Great guy. Thank you, Kim. Thank you. Very much. Great hand. Great. Thank you Russia. Thank you, Kimberl\n"
     ]
    }
   ],
   "source": [
    "# generate from the model\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "print(decode(m.generate(context, max_new_tokens=500)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df3622b1-1df8-4536-a7ce-af9f46984a45",
   "metadata": {},
   "source": [
    "# Just for Fun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "79eebb01-bc3d-4fc4-8aab-086dddf1ac15",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Number of Words: 370514\n"
     ]
    }
   ],
   "source": [
    "#Total number of words\n",
    "count = 0\n",
    "  \n",
    "# Opens a file in read mode \n",
    "f = open(\"trump.txt\", \"r\") \n",
    "  \n",
    "# Gets each line till end \n",
    "for line in f: \n",
    "    # Splits into words \n",
    "    word = line.split(\" \") \n",
    "    # Counts each words \n",
    "    count += len(word) \n",
    "\n",
    "total_words = count\n",
    "print(f'Total Number of Words: {total_words}') \n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d9e1afa5-0e21-4008-afcf-48e22f1a9d60",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique words in the file are: 8109\n"
     ]
    }
   ],
   "source": [
    "# Function to count  the number of unique words in the given text file.\n",
    "def countUniqueWords(fileName):\n",
    "    # Create a file object using open\n",
    "    # function and pass filename as parameter.\n",
    "    file = open(fileName, 'r')\n",
    "    # Read file contents as string and convert to lowercase.\n",
    "    read_file = file.read().lower()\n",
    "    words_in_file = read_file.split()  \n",
    "    # Creating a dictionary for counting number of occurrences.\n",
    "    count_map = {}\n",
    "    for i in words_in_file:\n",
    "        if i in count_map:\n",
    "            count_map[i] += 1  \n",
    "        else:\n",
    "            count_map[i] = 1\n",
    "    count = 0\n",
    "    # Traverse the dictionary and increment\n",
    "    # the counter for every unique word.\n",
    "    for i in count_map:\n",
    "        if count_map[i] == 1:\n",
    "            count += 1\n",
    "    file.close()\n",
    "    return count  # Return the count.\n",
    "\n",
    "unique_words = countUniqueWords('trump.txt')\n",
    "print('Number of unique words in the file are:', unique_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2595491c-d5cb-4bda-8662-e8952b92cae7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percent of unique words used in speeches: 2.1885812681841985\n"
     ]
    }
   ],
   "source": [
    "print(f'Percent of unique words used in speeches: {unique_words/total_words*100}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "62286ade-c2f4-4c19-9a9f-5abcd20d16d5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top Words Used:\n",
      "going 2319\n",
      "said 2217\n",
      "people 2144\n",
      "know 2039\n",
      "great 2005\n",
      "dont 1915\n",
      "right 1665\n",
      "like 1504\n",
      "thats 1490\n",
      "want 1489\n",
      "theyre 1439\n",
      "one 1394\n",
      "got 1230\n",
      "get 1180\n",
      "say 1150\n",
      "years 1111\n",
      "hes 1106\n",
      "think 1056\n",
      "never 1042\n",
      "thank 946\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "#nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "import string \n",
    "  \n",
    "# Open the file in read mode \n",
    "text = open(\"trump.txt\", \"r\") \n",
    "  \n",
    "# Create an empty dictionary \n",
    "d = dict() \n",
    "  \n",
    "# Loop through each line of the file \n",
    "for line in text: \n",
    "    # Remove the leading spaces and newline character \n",
    "    line = line.strip() \n",
    "  \n",
    "    # Convert the characters in line to \n",
    "    # lowercase to avoid case mismatch \n",
    "    line = line.lower() \n",
    "  \n",
    "    # Remove the punctuation marks from the line \n",
    "    line = line.translate(line.maketrans(\"\", \"\", string.punctuation)) \n",
    "  \n",
    "    # Split the line into words \n",
    "    words = line.split(\" \") \n",
    "    \n",
    "    # Remove stop words\n",
    "    stop_words = stopwords.words('english')\n",
    "    stopwords_dict = Counter(stop_words)\n",
    "    text = ' '.join([word for word in words if word not in stopwords_dict])\n",
    "    words = text.lower().split()  # Read, lowercase, and split into words\n",
    "  \n",
    "    # Iterate over each word in line \n",
    "    for word in words: \n",
    "        # Check if the word is already in dictionary \n",
    "        if word in d: \n",
    "            # Increment count of word by 1 \n",
    "            d[word] = d[word] + 1\n",
    "        else: \n",
    "            # Add the word to dictionary with count 1 \n",
    "            d[word] = 1\n",
    "  \n",
    "# Print the top used words\n",
    "d1 = {k: v for k, v in sorted(d.items(), reverse=True, key=lambda item: item[1])}\n",
    "\n",
    "from itertools import islice\n",
    "\n",
    "print('Top Words Used:')\n",
    "for key, value in islice(d1.items(), 20):\n",
    "    print(key, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbadef96-f2f2-45de-bd69-1d5047dc734a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
